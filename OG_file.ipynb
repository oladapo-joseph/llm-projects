{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "This notebook contains all practices I did, tests and things i tried out during the project.\n",
    "\n",
    "I will try to add more comments as I go on, they include things you can try for your projects\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "    pip install -r requirements\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The OPENAI module\n",
    "\n",
    "\n",
    "To use the OPENAI api, you need to create an account and get an API key, with the key you can use any OpenAI service or model\n",
    "\n",
    "\n",
    "Below are two ways out of many to use OPENAI LLM\n",
    "\n",
    "\n",
    "```\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "from langchain.schema import (AIMessage, HumanMessage, SystemMessage)\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "env = dotenv_values('.env')\n",
    "# Load environment variables from the .env file\n",
    "# Import necessary modules for working with OpenAI and LangChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_32648\\2382384196.py:1: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
      "  llm = OpenAI(model_name='text-davinci-003', api_key=env['OPENAI_API_KEY'])\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_32648\\2382384196.py:2: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  llm('Who is JFK in Ameria')\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Error code: 404 - {'error': {'message': 'The model `text-davinci-003` has been deprecated, learn more here: https://platform.openai.com/docs/deprecations', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotFoundError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m llm = OpenAI(model_name=\u001b[33m'\u001b[39m\u001b[33mtext-davinci-003\u001b[39m\u001b[33m'\u001b[39m, api_key=env[\u001b[33m'\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mllm\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mWho is JFK in Ameria\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\nlp projects\\llm\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:181\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    179\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    180\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\nlp projects\\llm\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:1301\u001b[39m, in \u001b[36mBaseLLM.__call__\u001b[39m\u001b[34m(self, prompt, stop, callbacks, tags, metadata, **kwargs)\u001b[39m\n\u001b[32m   1294\u001b[39m     msg = (\n\u001b[32m   1295\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mArgument `prompt` is expected to be a string. Instead found \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1296\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(prompt)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. If you want to run the LLM on multiple prompts, use \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1297\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`generate` instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1298\u001b[39m     )\n\u001b[32m   1299\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)  \u001b[38;5;66;03m# noqa: TRY004\u001b[39;00m\n\u001b[32m   1300\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m-> \u001b[39m\u001b[32m1301\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1302\u001b[39m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1303\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1304\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1305\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1306\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1307\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1308\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1309\u001b[39m     .generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]\n\u001b[32m   1310\u001b[39m     .text\n\u001b[32m   1311\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\nlp projects\\llm\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:966\u001b[39m, in \u001b[36mBaseLLM.generate\u001b[39m\u001b[34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    951\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m    952\u001b[39m     run_managers = [\n\u001b[32m    953\u001b[39m         callback_manager.on_llm_start(\n\u001b[32m    954\u001b[39m             \u001b[38;5;28mself\u001b[39m._serialized,\n\u001b[32m   (...)\u001b[39m\u001b[32m    964\u001b[39m         )\n\u001b[32m    965\u001b[39m     ]\n\u001b[32m--> \u001b[39m\u001b[32m966\u001b[39m     output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    969\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[32m    970\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\nlp projects\\llm\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:787\u001b[39m, in \u001b[36mBaseLLM._generate_helper\u001b[39m\u001b[34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[39m\n\u001b[32m    777\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate_helper\u001b[39m(\n\u001b[32m    778\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    779\u001b[39m     prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    783\u001b[39m     **kwargs: Any,\n\u001b[32m    784\u001b[39m ) -> LLMResult:\n\u001b[32m    785\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    786\u001b[39m         output = (\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[32m    791\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    794\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    795\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._generate(prompts, stop=stop)\n\u001b[32m    796\u001b[39m         )\n\u001b[32m    797\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    798\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\nlp projects\\llm\\Lib\\site-packages\\langchain_community\\llms\\openai.py:463\u001b[39m, in \u001b[36mBaseOpenAI._generate\u001b[39m\u001b[34m(self, prompts, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    451\u001b[39m     choices.append(\n\u001b[32m    452\u001b[39m         {\n\u001b[32m    453\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m: generation.text,\n\u001b[32m   (...)\u001b[39m\u001b[32m    460\u001b[39m         }\n\u001b[32m    461\u001b[39m     )\n\u001b[32m    462\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m463\u001b[39m     response = \u001b[43mcompletion_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_prompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    466\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    467\u001b[39m         \u001b[38;5;66;03m# V1 client returns the response in an PyDantic object instead of\u001b[39;00m\n\u001b[32m    468\u001b[39m         \u001b[38;5;66;03m# dict. For the transition period, we deep convert it to dict.\u001b[39;00m\n\u001b[32m    469\u001b[39m         response = response.dict()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\nlp projects\\llm\\Lib\\site-packages\\langchain_community\\llms\\openai.py:121\u001b[39m, in \u001b[36mcompletion_with_retry\u001b[39m\u001b[34m(llm, run_manager, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Use tenacity to retry the completion call.\"\"\"\u001b[39;00m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_openai_v1():\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m retry_decorator = _create_retry_decorator(llm, run_manager=run_manager)\n\u001b[32m    125\u001b[39m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_completion_with_retry\u001b[39m(**kwargs: Any) -> Any:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\nlp projects\\llm\\Lib\\site-packages\\openai\\_utils\\_utils.py:279\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    277\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    278\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\nlp projects\\llm\\Lib\\site-packages\\openai\\resources\\completions.py:539\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, model, prompt, best_of, echo, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, seed, stop, stream, stream_options, suffix, temperature, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    510\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    511\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    512\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    537\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    538\u001b[39m ) -> Completion | Stream[Completion]:\n\u001b[32m--> \u001b[39m\u001b[32m539\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    540\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    541\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    542\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    543\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbest_of\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    546\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mecho\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mecho\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    547\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    548\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    552\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    555\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msuffix\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    561\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    564\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    565\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    566\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    568\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    569\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mCompletion\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    570\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\nlp projects\\llm\\Lib\\site-packages\\openai\\_base_client.py:1242\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1228\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1229\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1230\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1237\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1238\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1239\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1240\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1241\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1242\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\nlp projects\\llm\\Lib\\site-packages\\openai\\_base_client.py:919\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[39m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    917\u001b[39m     retries_taken = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m919\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    921\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    922\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    923\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    924\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    925\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\nlp projects\\llm\\Lib\\site-packages\\openai\\_base_client.py:1023\u001b[39m, in \u001b[36mSyncAPIClient._request\u001b[39m\u001b[34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[39m\n\u001b[32m   1020\u001b[39m         err.response.read()\n\u001b[32m   1022\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1023\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_response(\n\u001b[32m   1026\u001b[39m     cast_to=cast_to,\n\u001b[32m   1027\u001b[39m     options=options,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1031\u001b[39m     retries_taken=retries_taken,\n\u001b[32m   1032\u001b[39m )\n",
      "\u001b[31mNotFoundError\u001b[39m: Error code: 404 - {'error': {'message': 'The model `text-davinci-003` has been deprecated, learn more here: https://platform.openai.com/docs/deprecations', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}"
     ]
    }
   ],
   "source": [
    "# Note: The model_name parameter has been deprecated. Consider using the newer ChatOpenAI or other updated classes.\n",
    "llm = OpenAI(model_name='text-davinci-003', api_key=env['OPENAI_API_KEY'])\n",
    "llm.invoke('Who is JFK in Ameria')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_32648\\3682594537.py:9: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = chat(message)\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "chat = ChatOpenAI(api_key=env['OPENAI_API_KEY'],\n",
    "                model='gpt-3.5-turbo',\n",
    "                temperature=0.3)\n",
    "message = [\n",
    "    SystemMessage(content='You are an expert data scientist'),\n",
    "    HumanMessage(content='Write a python script that trains a neural network on simulated data')\n",
    "]\n",
    "\n",
    "response = chat(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here is an example of a Python script that trains a simple neural network on simulated data using the TensorFlow library:\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import tensorflow as tf\n",
      "\n",
      "# Generate simulated data\n",
      "np.random.seed(0)\n",
      "X = np.random.rand(1000, 2)\n",
      "y = np.array([1 if x1 + x2 > 1 else 0 for x1, x2 in X])\n",
      "\n",
      "# Define the neural network architecture\n",
      "model = tf.keras.models.Sequential([\n",
      "    tf.keras.layers.Dense(4, activation='relu', input_shape=(2,)),\n",
      "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
      "])\n",
      "\n",
      "# Compile the model\n",
      "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
      "\n",
      "# Train the model\n",
      "model.fit(X, y, epochs=10, batch_size=32)\n",
      "\n",
      "# Evaluate the model\n",
      "loss, accuracy = model.evaluate(X, y)\n",
      "print(f'Loss: {loss}, Accuracy: {accuracy}')\n",
      "```\n",
      "\n",
      "In this script:\n",
      "1. We generate simulated data where the target variable is 1 if the sum of the input features is greater than 1, otherwise 0.\n",
      "2. We define a simple neural network with one hidden layer and one output layer using the TensorFlow Keras API.\n",
      "3. We compile the model with the Adam optimizer and binary cross-entropy loss.\n",
      "4. We train the model on the simulated data for 10 epochs.\n",
      "5. Finally, we evaluate the model on the training data and print the loss and accuracy.\n",
      "\n",
      "You can run this script in a Python environment with TensorFlow installed to train a neural network on simulated data.\n"
     ]
    }
   ],
   "source": [
    "print(response.content,end='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompting \n",
    "\n",
    "Using the PromptTemplate module, you can create custom prompts for specifici purposes \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate \n",
    "\n",
    "template = \"\"\"\n",
    "You are an expert data scientist with an expertise in building deep learning models. \n",
    "Explain the concept of {concept} in a couple of lines\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=['concept'],\n",
    "                        template = template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['concept'], input_types={}, partial_variables={}, template='\\nYou are an expert data scientist with an expertise in building deep learning models. \\nExplain the concept of {concept} in a couple of lines\\n')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Batch normalization is a technique used in deep learning to normalize the input of each layer by adjusting and scaling the activations. This helps in reducing internal covariate shift, improving training speed, and allowing for higher learning rates.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 44, 'prompt_tokens': 37, 'total_tokens': 81, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BE8D91DX7DQE1sDU9db1C440m0Fyp', 'finish_reason': 'stop', 'logprobs': None}, id='run-e12c5b83-4e19-4651-bab2-953fa2d07290-0', usage_metadata={'input_tokens': 37, 'output_tokens': 44, 'total_tokens': 81, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.invoke(prompt.format(concept='batch normalization'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LLMChains\n",
    "\n",
    "Similar to using a pipeline in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_32648\\2269629870.py:1: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  chain('bias and variance')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'concept': 'bias and variance',\n",
       " 'text': \"Bias refers to the error introduced by approximating a real-world problem, while variance refers to the error introduced by the model's sensitivity to fluctuations in the training data. Balancing bias and variance is crucial for creating a model that generalizes well to unseen data.\"}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain('bias and variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'concept': 'bias and variance', 'text': 'Bias refers to the error introduced by approximating a real-world problem, while variance refers to the error introduced by sensitivity to fluctuations in the training data. Balancing bias and variance is crucial in building accurate and generalizable deep learning models.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain \n",
    "\n",
    "chain = LLMChain(llm=chat, prompt= prompt) \n",
    "\n",
    "print(chain.invoke('bias and variance'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SimpleSequential Chains\n",
    "\n",
    "helps to line up a series of prompts and activities you want to do, \n",
    "when you want the result of a prompt to be used as an input to another prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain \n",
    "\n",
    "template2 = \"\"\" you are an expert data scientist. Turn the description of {ml_concept} and explain as if to a toddler in 300 words\"\"\"\n",
    "prompt2 = PromptTemplate(\n",
    "        input_variables=['ml_concept'],\n",
    "        template = template2\n",
    ")\n",
    "chain2 = LLMChain(llm=chat, prompt=prompt2)\n",
    "\n",
    "all_chain = SimpleSequentialChain(chains=[chain,chain2], verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mBias refers to the error introduced by approximating a real-world problem, leading to underfitting. Variance refers to the error introduced by modeling the noise in the training data, leading to overfitting. Balancing bias and variance is crucial for building accurate and generalizable deep learning models.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mImagine you are trying to build a puzzle. Bias is like when you try to put a piece in the wrong spot because you don't really understand what the picture is supposed to look like. This can make your puzzle look really messy and not quite right. \n",
      "\n",
      "Variance is like when you try to make the puzzle perfect by putting every piece exactly where it seems to fit, even if it doesn't really belong there. This can make your puzzle look too perfect and not like the original picture at all.\n",
      "\n",
      "When you are building a puzzle, you want to find a balance between making sure each piece fits correctly and not forcing pieces where they don't belong. This is similar to balancing bias and variance in deep learning models.\n",
      "\n",
      "If you have too much bias, your model won't be able to understand the real problem you are trying to solve. If you have too much variance, your model will try to fit every little detail in the training data, even if it's just noise and not important.\n",
      "\n",
      "To build a good deep learning model, you need to find the right balance between bias and variance. This means making sure your model understands the problem well enough without getting too caught up in the little details. By finding this balance, you can create a model that is accurate and can work well with new data it hasn't seen before.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'bias and variance', 'output': \"Imagine you are trying to build a puzzle. Bias is like when you try to put a piece in the wrong spot because you don't really understand what the picture is supposed to look like. This can make your puzzle look really messy and not quite right. \\n\\nVariance is like when you try to make the puzzle perfect by putting every piece exactly where it seems to fit, even if it doesn't really belong there. This can make your puzzle look too perfect and not like the original picture at all.\\n\\nWhen you are building a puzzle, you want to find a balance between making sure each piece fits correctly and not forcing pieces where they don't belong. This is similar to balancing bias and variance in deep learning models.\\n\\nIf you have too much bias, your model won't be able to understand the real problem you are trying to solve. If you have too much variance, your model will try to fit every little detail in the training data, even if it's just noise and not important.\\n\\nTo build a good deep learning model, you need to find the right balance between bias and variance. This means making sure your model understands the problem well enough without getting too caught up in the little details. By finding this balance, you can create a model that is accurate and can work well with new data it hasn't seen before.\"}\n"
     ]
    }
   ],
   "source": [
    "response = all_chain.invoke('bias and variance')\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents \n",
    "\n",
    "Basically AI bots for performing specific tasks \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.agents.agent_toolkits.python.base import create_python_agent\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from langchain_experimental.tools.python.tool import PythonREPLTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "agentexecutor = create_python_agent(\n",
    "    llm= OpenAI(api_key=env['OPENAI_API_KEY'],\n",
    "                temperature= 0, max_tokens= 1000),\n",
    "    tool= PythonREPLTool(),\n",
    "    verbose= True \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python REPL can execute arbitrary code. Use with caution.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m I can use the quadratic formula to find the roots of a quadratic equation.\n",
      "Action: Python_REPL\n",
      "Action Input: (-4 + (4**2 - 4*3*2)**0.5) / (2*3)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I can use the quadratic formula to find the roots of a quadratic equation.\n",
      "Action: Python_REPL\n",
      "Action Input: (-4 - (4**2 - 4*3*2)**0.5) / (2*3)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: -0.3333333333333333, -0.6666666666666666\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Find the roots of a quadratic equation funcion 3x^6 + 4x + 2 = 1',\n",
       " 'output': '-0.3333333333333333, -0.6666666666666666'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agentexecutor.invoke('Find the roots of a quadratic equation funcion 3x^6 + 4x + 2 = 1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using exec allows you to be able to use ai agents to run python codes, since the responses come in strings\n",
    "> There are two ways to do this \n",
    "1. exec can run strings as seen below and still capture the output of the codes \n",
    "2. using subprocess.run (this can even give you access to the shell but not advisable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = \"\"\" \n",
    "def greet(f):\n",
    "    return f'Hello {f}' \n",
    "new = greet('Dapo')\n",
    "\"\"\"\n",
    "namespace = {}\n",
    "exec(code, namespace) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Dapo\n"
     ]
    }
   ],
   "source": [
    "print(namespace['new'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world\n"
     ]
    }
   ],
   "source": [
    "import subprocess \n",
    "\n",
    "result = subprocess.run(['python', '-c', 'print(\"Hello world\")'],\n",
    "                        capture_output=True,text=True) \n",
    "\n",
    "print(result.stdout.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing out python code generation\n",
    "\n",
    "# first step is to have something to take in an input and prompt\n",
    "# then code generation output\n",
    "# the code is then passed as an input into the agent that calls the exec function to run the code using the input variable \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI \n",
    "from langchain import PromptTemplate \n",
    "from langchain.chains import LLMChain \n",
    "\n",
    "\n",
    "chat = OpenAI(api_key=env['OPENAI_API_KEY'], temperature=0.2,\n",
    "              max_tokens= 1000)\n",
    "\n",
    "template3 = \"\"\" \n",
    "        You are an expert data scientist.\n",
    "        Create a python script that takes in an integer argument {number} and returns the number mupltiplied by 4, \n",
    "        then create a variable called answer to store the value of the function\n",
    "\"\"\"\n",
    "\n",
    "prompt3 = PromptTemplate(input_variables= ['number'], \n",
    "                         template= template3,\n",
    "                         )\n",
    "\n",
    "chain3 = LLMChain(llm=chat, prompt=prompt3)\n",
    "\n",
    "response = chain3.invoke(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "namespace = {}\n",
    "exec(response['text'].strip(),namespace )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "namespace['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import  ConversationBufferMemory,ConversationBufferWindowMemory\n",
    "from langchain_openai import ChatOpenAI \n",
    "from langchain.chains import ConversationChain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv \n",
    "\n",
    "load_dotenv('.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = ConversationChain(llm=ChatOpenAI(temperature=0.3))\n",
    "                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI:'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.prompt.template "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'bias and variance',\n",
       " 'history': '',\n",
       " 'response': \"Ah, bias and variance are key concepts in machine learning! Bias refers to the error introduced by approximating a real-world problem, while variance is the error introduced by modeling the noise in the training data. Balancing bias and variance is crucial for building a model that generalizes well to unseen data. It's like finding the sweet spot between underfitting and overfitting. Do you want to know more about how to manage bias and variance in machine learning models?\"}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'input':'bias and variance'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': \"argentina's last match was against who and when?\",\n",
       " 'history': \"Human: bias and variance\\nAI: Ah, bias and variance are key concepts in machine learning! Bias refers to the error introduced by approximating a real-world problem, while variance is the error introduced by modeling the noise in the training data. Balancing bias and variance is crucial for building a model that generalizes well to unseen data. It's like finding the sweet spot between underfitting and overfitting. Do you want to know more about how to manage bias and variance in machine learning models?\",\n",
       " 'response': \"Argentina's last match was against Brazil on July 10, 2021. They played in the final of the Copa America tournament. Argentina won the match 1-0, with Angel Di Maria scoring the only goal of the game. It was a historic victory for Argentina, as it was their first Copa America title since 1993.\"}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"argentina's last match was against who and when?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: bias and variance\n",
      "AI: Ah, bias and variance are key concepts in machine learning! Bias refers to the error introduced by approximating a real-world problem, while variance is the error introduced by modeling the noise in the training data. Balancing bias and variance is crucial for building a model that generalizes well to unseen data. It's like finding the sweet spot between underfitting and overfitting. Do you want to know more about how to manage bias and variance in machine learning models?\n",
      "Human: argentina's last match was against who and when?\n",
      "AI: Argentina's last match was against Brazil on July 10, 2021. They played in the final of the Copa America tournament. Argentina won the match 1-0, with Angel Di Maria scoring the only goal of the game. It was a historic victory for Argentina, as it was their first Copa America title since 1993.\n"
     ]
    }
   ],
   "source": [
    "print(chain.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings \n",
    "\n",
    "---\n",
    "\n",
    "Using embeddings, OllamaEmbeddings to save money, llama3.2 to also run locally \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS \n",
    "from langchain_ollama import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_texts(['hello world', 'hello world 2', \"hi there\"], \n",
    "                 embedding=OllamaEmbeddings(model='llama3.2')\n",
    "                \n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi there\n",
      "hello world\n",
      "hello world 2\n"
     ]
    }
   ],
   "source": [
    "# Perform a similarity search\n",
    "query = \"hello\"\n",
    "results = db.similarity_search(query)  # k is the number of top results to retrieve\n",
    "\n",
    "# Print the results\n",
    "for result in results:\n",
    "    print(result.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='dba82005-d6f9-4dbe-997c-413eec0fd272', metadata={}, page_content='hi there'),\n",
       " Document(id='df3f65a9-7bbf-4b58-ba59-ca818be1f42d', metadata={}, page_content='hello world'),\n",
       " Document(id='63b197ab-e275-47e2-9ae4-b76d821c6308', metadata={}, page_content='hello world 2')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saving embeddings\n",
    "\n",
    "It saves locally, and can be pointed in a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.save_local('faiss_index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also load the old vectorstore and update it with new data\n",
    "\n",
    "you have to merge and save when you are done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdb = FAISS.load_local('faiss_index', OllamaEmbeddings(model='gemma3'), allow_dangerous_deserialization=True)\n",
    "\n",
    "db2 = FAISS.from_texts(['food is good','my guy how are you', 'hello there'],\n",
    "                 embedding=OllamaEmbeddings(model='gemma3')\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdb.merge_from(db2)\n",
    "newdb.save_local('faiss_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='ba9a0929-3f3a-4a52-816f-9b6a270f21ea', metadata={}, page_content='hello there'),\n",
       " Document(id='dba82005-d6f9-4dbe-997c-413eec0fd272', metadata={}, page_content='hi there'),\n",
       " Document(id='df3f65a9-7bbf-4b58-ba59-ca818be1f42d', metadata={}, page_content='hello world'),\n",
       " Document(id='c2e1ea99-281c-4dfa-a345-097cecbc0fc8', metadata={}, page_content='my guy how are you')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdb.similarity_search('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='bdf4c390-865b-4bbf-ad03-b6091f67bd8c', metadata={}, page_content='food is good'),\n",
       " Document(id='ba9a0929-3f3a-4a52-816f-9b6a270f21ea', metadata={}, page_content='hello there'),\n",
       " Document(id='c2e1ea99-281c-4dfa-a345-097cecbc0fc8', metadata={}, page_content='my guy how are you'),\n",
       " Document(id='63b197ab-e275-47e2-9ae4-b76d821c6308', metadata={}, page_content='hello world 2')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdb.similarity_search('food')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another way to do embedding using Llama index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\Desktop\\nlp projects\\llm\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, Document\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "# from langchain_community.document_loaders import PyPDFLoader\n",
    "# load data()\n",
    "loader = SimpleDirectoryReader(\n",
    "    input_dir='data',\n",
    "    required_exts=['.pdf'],\n",
    "    recursive=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs= loader.load_data()\n",
    "embed_model = HuggingFaceEmbedding( model_name=\"BAAI/bge-large-en-v1.5\", trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "# ====== Create vector store and upload indexed data ======\n",
    "Settings.embed_model = embed_model # we specify the embedding model to be used\n",
    "index = VectorStoreIndex.from_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_Settings(_llm=None, _embed_model=HuggingFaceEmbedding(model_name='BAAI/bge-large-en-v1.5', embed_batch_size=10, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x000001F360646010>, num_workers=None, max_length=512, normalize=True, query_instruction=None, text_instruction=None, cache_folder=None), _callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x000001F360646010>, _tokenizer=None, _node_parser=SentenceSplitter(include_metadata=True, include_prev_next_rel=True, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x000001F360646010>, id_func=<function default_id_func at 0x000001F350649BC0>, chunk_size=1024, chunk_overlap=200, separator=' ', paragraph_separator='\\n\\n\\n', secondary_chunking_regex='[^,.;]+[,.;]?|[,.;]'), _prompt_helper=None, _transformations=[SentenceSplitter(include_metadata=True, include_prev_next_rel=True, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x000001F360646010>, id_func=<function default_id_func at 0x000001F350649BC0>, chunk_size=1024, chunk_overlap=200, separator=' ', paragraph_separator='\\n\\n\\n', secondary_chunking_regex='[^,.;]+[,.;]?|[,.;]')])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "# setting up the llm\n",
    "\n",
    "llm = Ollama(model=\"gemma3\", request_timeout=500) \n",
    "\n",
    "# ====== Setup a query engine on the index previously created ======\n",
    "Settings.llm = llm # specifying the llm to be used\n",
    "query_engine = index.as_query_engine(streaming=True, similarity_top_k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.prompts import PromptTemplate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_prompt_tmpl_str = (\n",
    "            \"\"\"Context information is below.\\n\"\n",
    "            ---------------------\n",
    "            {context_str}\n",
    "            ---------------------\n",
    "            Given the context information above I want you to think step by step to answer the query in a crisp manner, \n",
    "            incase case you don't know the answer say 'I don't know!'.\\n\n",
    "            Query: {query_str}\\n\"\n",
    "            \"Answer: \n",
    "            \"\"\")\n",
    "\n",
    "qa_prompt_tmpl = PromptTemplate(qa_prompt_tmpl_str)\n",
    "query_engine.update_prompts({\"response_synthesizer:text_qa_template\": qa_prompt_tmpl})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document is about the admission requirements and procedures for the Winter 2025-2026 Master in Data Science program at the University of Luxembourg, specifically for third-country national applicants. It details required documents, application deadlines, tuition fees, and contact information.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query('What is the document about?')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document loaders in details \n",
    "\n",
    "\n",
    "\n",
    "Still on Embeddings \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader,CSVLoader,JSONLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader('data/onedrive.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data/onedrive.txt'}, page_content='Microsoft OneDrive is a cloud storage service that allows users to store files and access them from anywhere with an internet connection. It is integrated with the Microsoft 365 ecosystem, making it a convenient choice for individuals and businesses.\\n\\n**Benefits:**\\n1. **Accessibility:** Files stored in OneDrive can be accessed from any device, including PCs, smartphones, and tablets.\\n2. **Collaboration:** OneDrive enables real-time collaboration on documents, spreadsheets, and presentations through Microsoft Office apps.\\n3. **Backup and Sync:** It provides automatic backup and synchronization of files across devices, ensuring data is always up-to-date.\\n4. **Security:** OneDrive offers robust security features, including encryption, ransomware detection, and recovery options.\\n5. **Integration:** Seamless integration with Microsoft 365 apps like Word, Excel, and Teams enhances productivity.\\n\\n**Use Cases:**\\n- **Personal Use:** Storing photos, videos, and important documents for easy access and sharing.\\n- **Business Use:** Collaborating on projects, sharing files with team members, and managing workflows.\\n- **Education:** Sharing study materials, assignments, and collaborating on group projects.\\n- **Backup Solution:** Safeguarding critical data by storing it in the cloud.\\n\\n**Pros:**\\n- Easy to use and integrates well with Microsoft products.\\n- Offers free storage with additional paid plans for more space.\\n- Supports offline access to files.\\n- Advanced sharing options with permissions control.\\n\\n**Cons:**\\n- Limited free storage compared to some competitors.\\n- Requires a stable internet connection for optimal performance.\\n- Advanced features may require a Microsoft 365 subscription.\\n\\nOverall, Microsoft OneDrive is a versatile and reliable cloud storage solution that caters to both personal and professional needs.')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_loader= CSVLoader('data/trades.csv', source_column='Company')\n",
    "# the source column is the column that contains the source of the document being retrieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_docs = csv_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "975"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(csv_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'Stevens-Brown', 'row': 0}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_docs[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt to fetch exchange rates from Aboki FX "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredURLLoader\n",
    "\n",
    "url_loader = UnstructuredURLLoader(urls= ['https://abokiforex.app/'])\n",
    "url_docs = url_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://abokiforex.app/'}, page_content=\"Aboki Forex - Naira to Dollar Black Market Today\\n\\nDollar to Naira Today Black Market\\n\\nThe Black Market Dollar to Naira Rates are tabulated below:\\n\\nBlack Market Rates\\n\\nDollar to Naira rate\\n\\nnaira to dollar\\n\\nBUY\\n\\ndollar to naira\\n\\n1540\\n\\nDOLLAR (USD)\\n\\nSELL\\n\\npound to dollar\\n\\n1550\\n\\nPound to Naira rate\\n\\ndollar to pound\\n\\nBUY\\n\\ndollar to yen\\n\\n1990\\n\\nPOUND (GBP)\\n\\nSELL\\n\\npound to euro\\n\\n2020\\n\\nEuro to Naira rate\\n\\neuro to pound\\n\\nBUY\\n\\ndollar to euro\\n\\n1660\\n\\nEURO (EUR)\\n\\nSELL\\n\\ndollar to euro\\n\\n1690\\n\\nCanadian Dollar to Naira rate\\n\\ncanadian dollar to euro\\n\\nBUY\\n\\ndollar to canadian dollar\\n\\n1000\\n\\nDOLLAR (CAD)\\n\\nSELL\\n\\ndollar to rand\\n\\n1150\\n\\nSouth African Rand to Naira rate\\n\\nRand to dollar\\n\\nBUY\\n\\nzar to dollar\\n\\n80\\n\\nRAND (ZAR)\\n\\nSELL\\n\\nyuan to dollar\\n\\n100\\n\\nUAE Dirham to Naira rate\\n\\ndirham aed to dollar\\n\\nBUY\\n\\ndollar to yuan\\n\\n380\\n\\nDIRHAM (AED)\\n\\nSELL\\n\\ndirham to dollar\\n\\n420\\n\\nChinese Yuan to Naira rate\\n\\nyuan to euro\\n\\nBUY\\n\\neuro to dirham\\n\\n190\\n\\nYUAN (CNY)\\n\\nSELL\\n\\npound to yuan\\n\\n215\\n\\nGhanaian Cedi to Naira rate\\n\\nGhanaian cedi to dollar\\n\\nBUY\\n\\ndollar to cedi\\n\\n90\\n\\nG.CEDI (GHS)\\n\\nSELL\\n\\npound to yuan\\n\\n105\\n\\nWest African CFA franc to Naira rate\\n\\nxof to dollar\\n\\nBUY\\n\\ndollar to xof\\n\\n2370\\n\\nCFA F.(XOF)\\n\\nSELL\\n\\ndollar to xaf\\n\\n2550\\n\\nCentral African CFA franc to Naira rate\\n\\nxaf to dollar\\n\\nBUY\\n\\ndollar to xaf\\n\\n2250\\n\\nCFA F.(XAF)\\n\\nSELL\\n\\neuro to xaf\\n\\n2400\\n\\nAustralian Dollar (Aussie) to Naira rate\\n\\naussie to dollar\\n\\nBUY\\n\\naussie to naira\\n\\n850\\n\\nAUSSIE (AUD)\\n\\nSELL\\n\\naustrailian dollar to naira\\n\\n1000\\n\\nCBN Rates - Dollars to Nairas, Pounds to Nairas, Euros to Nairas, and Others\\n\\nCentral Bank of Nigeria Rates\\n\\nCBN Dollar to Naira rate\\n\\ndollar\\n\\nDOLLAR (USD)\\n\\ndollar\\n\\n1536.82\\n\\nCBN Pound to Naira rate\\n\\npound sterling\\n\\nPOUND (GBP)\\n\\ndollar\\n\\n1991.87\\n\\nCBN Euro to Naira rate\\n\\neuro\\n\\nEURO (EUR)\\n\\ndollar\\n\\n1660.99\\n\\nCBN Swiss Franc to Naira rate\\n\\neuro\\n\\nSWISS FRANC (CHF)\\n\\ndollar\\n\\n1741.83\\n\\nCBN Japanese Yen to Naira rate\\n\\neuro\\n\\nJAPANESE YEN (JPN)\\n\\ndollar\\n\\n10.20\\n\\nCBN West African CFA Franc to Naira rate\\n\\ncfa franc\\n\\nCFA FRANC (XOF)\\n\\ndollar\\n\\n2.53\\n\\nCBN West African Unit of Account to Naira rate\\n\\neuro\\n\\nWEST AFRICAN UNIT OF ACCOUNT (WAUA)\\n\\ndollar\\n\\n2040.43\\n\\nCBN Chinese Yuan to Naira rate\\n\\neuro\\n\\nCHINESE YUAN (CNY)\\n\\ndollar\\n\\n211.51\\n\\nCBN Saudi Riyal to Naira rate\\n\\neuro\\n\\nSAUDI RIYAL (SAR)\\n\\ndollar\\n\\n409.70\\n\\nCBN South African Rand to Naira rate\\n\\neuro\\n\\nSOUTH AFRICAN RAND (ZAR)\\n\\ndollar\\n\\n84.68\\n\\nBlack Market Exchange Rate Currency Converter\\n\\nEmbed this widget on your site\\n\\nCopy and paste the following code into your website to display this currency converter widget:\\n\\nPowered by Aboki Forex\\n\\nForex News\\n\\nOgun & Morocco: A Dynamic Duo for Energy, Agriculture, and Seaports\\n\\nABOKI FOREX\\n\\n30 sec ago\\n\\nNigeria Cheers as AfDB Accelerates Economic Growth  Yes, You Heard That Right!\\n\\nABOKI FOREX\\n\\n1 hrs ago\\n\\nCheers to the Good Times: U.S. Wine Exports to Nigeria Hit $7.8 Million in 2024!\\n\\nABOKI FOREX\\n\\n2 hrs ago\\n\\nTony Elumelu: IMF's New Secret Weapon in Entrepreneurship!\\n\\nABOKI FOREX\\n\\n3 hrs ago\\n\\nTurning the Tide: How Tinubu's Economic Agenda is Making Waves\\n\\nABOKI FOREX\\n\\n4 hrs ago\\n\\nNaira's Roller Coaster Ride: Falling Officially, Rising Underground!\\n\\nABOKI FOREX\\n\\n20 hrs ago\\n\\nThe Naira's Not-So-Fabulous Journey: Forbes Weighs In\\n\\nABOKI FOREX\\n\\n21 hrs ago\\n\\nCurrency Chronicles: A Steady Day for Most, but Zambia's in a Spin!\\n\\nABOKI FOREX\\n\\n22 hrs ago\\n\\nEuro Takes a Tumble as the Dollar Struts Its Stuff Amid Inflation Woes\\n\\nABOKI FOREX\\n\\n23 hrs ago\\n\\nDollar Dances with Mixed Feelings as Peso and Loonie Take a Hit from Auto Tariffs\\n\\nABOKI FOREX\\n\\n1 days ago\\n\\nConvert US Dollar to Naira Exchange Rate in the Black Market?\\n\\nHere is an easy access to convert the United States Dollar (USD) to Nigerian Naira using the Black Market Rates\\n\\nAlso you can convert the Nigerian Naira to United States Dollar (USD) Currency using this currency converter\\n\\nDollar to Naira Rate Currency Conversion Table\\n\\nAmount in Dollar ($) Black Market Exchange Rate in Naira 1 US Dollar ($1) to Naira 1,540 5 US Dollars ($5) to Naira 7,700 10 US Dollars ($10) to Naira 15,400 20 US Dollars ($20) to Naira 30,800 50 US Dollars ($50) to Naira 77,000 100 US Dollars ($100) to Naira 154,000 200 US Dollars ($200) to Naira 308,000 300 US Dollars ($300) to Naira 462,000 400 US Dollars ($400) to Naira 616,000 500 US Dollars ($500) to Naira 770,000 600 US Dollars ($600) to Naira 924,000 800 US Dollars ($800) to Naira 1,232,000 1000 US Dollars ($1000) to Naira 1,540,000 5000 US Dollars ($5000) to Naira 7,700,000 10000 US Dollars ($10000) to Naira 15,400,000\\n\\nQ: Does Aboki Forex Trade (Exchange) Currencies?\\n\\nA: No, we do not trade any currency pair.\\n\\nQ: Is AbokiFx the same with Aboki Forex?\\n\\nA: No, This is Aboki Forex and we are not affiliated with Abokifx.\\n\\nQ: How can I Exchange Currency?\\n\\nA: Contact your local market or bank.\\n\\nQ: Are the Black Market Rates accurate in all Markets?\\n\\nA: No, we try to provide the average rates in the local markets. Since the market is not regulated prices will vary.\\n\\nQ: What is the difference between CBN, I&E and Black Market Rates?\\n\\nA: The CBN rates are now the same with the I&E rates. The Black Market Rates differs.\\n\\nMobile apps?\\n\\nIt's even better with our mobile apps.\\n\\napp-store\\n\\ngoogle play\\n\\nCopyright  2025 GreatCallie\")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using openai LLM to extract the key data using a very solid prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "# chatmodel = ChatOpenAI()\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate \n",
    "\n",
    "llm = ChatOpenAI(temperature=0.4)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "        input_variables=['file'],\n",
    "        template= '''\n",
    "            You are a very good webscraper, also good with data mining.\n",
    "\n",
    "            Fetch the exchange rate of the currencies in the document and return the result in a dictionary format\n",
    "            for both the official rate and the black market.\n",
    "            The document is as follows: {file}\n",
    "'''\n",
    "    )\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "response = chain.invoke({'file':url_docs[0].page_content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"exchange_rates\": {\n",
      "        \"black_market\": {\n",
      "            \"USD\": {\n",
      "                \"buy\": 1540,\n",
      "                \"sell\": 1550\n",
      "            },\n",
      "            \"GBP\": {\n",
      "                \"buy\": 2020,\n",
      "                \"sell\": 2020\n",
      "            },\n",
      "            \"EUR\": {\n",
      "                \"buy\": 1690,\n",
      "                \"sell\": 1690\n",
      "            },\n",
      "            \"CAD\": {\n",
      "                \"buy\": 1000,\n",
      "                \"sell\": 1000\n",
      "            },\n",
      "            \"ZAR\": {\n",
      "                \"buy\": 80,\n",
      "                \"sell\": 1150\n",
      "            },\n",
      "            \"AED\": {\n",
      "                \"buy\": 380,\n",
      "                \"sell\": 420\n",
      "            },\n",
      "            \"CNY\": {\n",
      "                \"buy\": 190,\n",
      "                \"sell\": 215\n",
      "            },\n",
      "            \"GHS\": {\n",
      "                \"buy\": 90,\n",
      "                \"sell\": 105\n",
      "            },\n",
      "            \"XOF\": {\n",
      "                \"buy\": 2370,\n",
      "                \"sell\": 2550\n",
      "            },\n",
      "            \"XAF\": {\n",
      "                \"buy\": 2250,\n",
      "                \"sell\": 2400\n",
      "            },\n",
      "            \"AUD\": {\n",
      "                \"buy\": 850,\n",
      "                \"sell\": 1000\n",
      "            }\n",
      "        },\n",
      "        \"official\": {\n",
      "            \"USD\": 1536.82,\n",
      "            \"GBP\": 1991.87,\n",
      "            \"EUR\": 1660.99,\n",
      "            \"CHF\": 1741.83,\n",
      "            \"JPY\": 10.20,\n",
      "            \"XOF\": 2.53,\n",
      "            \"WAUA\": 2040.43,\n",
      "            \"CNY\": 211.51,\n",
      "            \"SAR\": 409.70,\n",
      "            \"ZAR\": 84.68\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM \n",
    "\n",
    "ollama_llm = OllamaLLM(model='llama3.2', temperature=0.2, max_tokens=1000)\n",
    "\n",
    "ollama_chain = prompt | ollama_llm | StrOutputParser()\n",
    "response = ollama_chain.invoke({'file':url_docs[0].page_content})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import re\n",
      "\n",
      "# Define the document content\n",
      "document = \"\"\"\n",
      "Dollar to Naira Today Black Market\n",
      "\n",
      "The Black Market Dollar to Naira Rates are tabulated below:\n",
      "\n",
      "Black Market Rates\n",
      "\n",
      "Dollar to Naira rate\n",
      "\n",
      "naira to dollar\n",
      "\n",
      "BUY\n",
      "\n",
      "dollar to naira\n",
      "\n",
      "1540\n",
      "\n",
      "DOLLAR (USD)\n",
      "\n",
      "SELL\n",
      "\n",
      "pound to dollar\n",
      "\n",
      "1550\n",
      "\n",
      "Pound to Naira rate\n",
      "\n",
      "dollar to pound\n",
      "\n",
      "BUY\n",
      "\n",
      "dollar to yen\n",
      "\n",
      "1990\n",
      "\n",
      "POUND (GBP)\n",
      "\n",
      "SELL\n",
      "\n",
      "pound to euro\n",
      "\n",
      "2020\n",
      "\n",
      "Euro to Naira rate\n",
      "\n",
      "euro to pound\n",
      "\n",
      "BUY\n",
      "\n",
      "dollar to euro\n",
      "\n",
      "1660\n",
      "\n",
      "EURO (EUR)\n",
      "\n",
      "SELL\n",
      "\n",
      "dollar to euro\n",
      "\n",
      "1690\n",
      "\n",
      "Canadian Dollar to Naira rate\n",
      "\n",
      "canadian dollar to euro\n",
      "\n",
      "BUY\n",
      "\n",
      "dollar to canadian dollar\n",
      "\n",
      "1000\n",
      "\n",
      "DOLLAR (CAD)\n",
      "\n",
      "SELL\n",
      "\n",
      "dollar to rand\n",
      "\n",
      "1150\n",
      "\n",
      "South African Rand to Naira rate\n",
      "\n",
      "Rand to dollar\n",
      "\n",
      "BUY\n",
      "\n",
      "zar to dollar\n",
      "\n",
      "80\n",
      "\n",
      "RAND (ZAR)\n",
      "\n",
      "SELL\n",
      "\n",
      "yuan to dollar\n",
      "\n",
      "100\n",
      "\n",
      "UAE Dirham to Naira rate\n",
      "\n",
      "dirham aed to dollar\n",
      "\n",
      "BUY\n",
      "\n",
      "dollar to yuan\n",
      "\n",
      "380\n",
      "\n",
      "DIRHAM (AED)\n",
      "\n",
      "SELL\n",
      "\n",
      "dirham to dollar\n",
      "\n",
      "420\n",
      "\n",
      "Chinese Yuan to Naira rate\n",
      "\n",
      "yuan to euro\n",
      "\n",
      "BUY\n",
      "\n",
      "euro to dirham\n",
      "\n",
      "190\n",
      "\n",
      "YUAN (CNY)\n",
      "\n",
      "SELL\n",
      "\n",
      "pound to yuan\n",
      "\n",
      "215\n",
      "\n",
      "Ghanaian Cedi to Naira rate\n",
      "\n",
      "Ghanaian cedi to dollar\n",
      "\n",
      "BUY\n",
      "\n",
      "dollar to cedi\n",
      "\n",
      "90\n",
      "\n",
      "G.CEDI (GHS)\n",
      "\n",
      "SELL\n",
      "\n",
      "pound to yuan\n",
      "\n",
      "105\n",
      "\n",
      "West African CFA franc to Naira rate\n",
      "\n",
      "xof to dollar\n",
      "\n",
      "BUY\n",
      "\n",
      "dollar to xof\n",
      "\n",
      "80\n",
      "\n",
      "WEST AFRICAN CFA FRANC (XOF)\n",
      "\n",
      "SELL\n",
      "\n",
      "xof to euro\n",
      "\n",
      "100\n",
      "\n",
      "EURO (EUR)\n",
      "\n",
      "CBN Rates:\n",
      "\n",
      "1 USD = 193.5 Naira\n",
      "1 EUR = 204.8 Naira\n",
      "1 CAD = 380.9 Naira\n",
      "1 ZAR = 20.6 Naira\n",
      "1 AED = 4.2 Naira\n",
      "1 CNY = 7.3 Naira\n",
      "1 GHS = 5.5 Naira\n",
      "\n",
      "I&E Rates:\n",
      "\n",
      "1 USD = 193.8 Naira\n",
      "1 EUR = 204.9 Naira\n",
      "1 CAD = 380.9 Naira\n",
      "1 ZAR = 20.6 Naira\n",
      "1 AED = 4.2 Naira\n",
      "1 CNY = 7.3 Naira\n",
      "1 GHS = 5.5 Naira\n",
      "\n",
      "Dollar to Naira Exchange Rate in the Black Market?\n",
      "\n",
      "Here is an easy access to convert the United States Dollar (USD) to Nigerian Naira using the Black Market Rates\n",
      "\n",
      "Also you can convert the Nigerian Naira to United States Dollar (USD) Currency using this currency converter\n",
      "\n",
      "Dollar to Naira Rate Currency Conversion Table\n",
      "\n",
      "Amount in Dollar ($) Black Market Exchange Rate in Naira 1 US Dollar ($1) to Naira 1,540 5 US Dollars ($5) to Naira 7,700 10 US Dollars ($10) to Naira 15,400 20 US Dollars ($20) to Naira 30,800 50 US Dollars ($50) to Naira 77,000 100 US Dollars ($100) to Naira 154,000 200 US Dollars ($200) to Naira 308,000 300 US Dollars ($300) to Naira 462,000 400 US Dollars ($400) to Naira 616,000 500 US Dollars ($500) to Naira 770,000 600 US Dollars ($600) to Naira 924,000 800 US Dollars ($800) to Naira 1,232,000 1000 US Dollars ($1000) to Naira 1,540,000 5000 US Dollars ($5000) to Naira 7,700,000 10000 US Dollars ($10000) to Naira 15,400,000\n",
      "\"\"\"\n",
      "\n",
      "# Use regular expressions to extract the exchange rates\n",
      "rates = re.findall(r'(\\d+\\.\\d+) USD = (\\d+\\.\\d+)', document)\n",
      "\n",
      "# Create a dictionary with the exchange rates\n",
      "exchange_rates = {\n",
      "    'USD': {'black_market': float(rates[0][1]), 'cbn': float(rates[1][1]), 'i&e': float(rates[2][1])},\n",
      "    'EUR': {'black_market': float(rates[3][1]), 'cbn': float(rates[4][1]), 'i&e': float(rates[5][1])},\n",
      "    'CAD': {'black_market': float(rates[6][1]), 'cbn': float(rates[7][1]), 'i&e': float(rates[8][1])},\n",
      "    'ZAR': {'black_market': float(rates[9][1]), 'cbn': float(rates[10][1]), 'i&e': float(rates[11][1])},\n",
      "    'AED': {'black_market': float(rates[12][1]), 'cbn': float(rates[13][1]), 'i&e': float(rates[14][1])},\n",
      "    'CNY': {'black_market': float(rates[15][1]), 'cbn': float(rates[16][1]), 'i&e': float(rates[17][1])},\n",
      "    'GHS': {'black_market': float(rates[18][1]), 'cbn': float(rates[19][1]), 'i&e': float(rates[20][1])}\n",
      "}\n",
      "\n",
      "# Print the exchange rates\n",
      "for currency, rates in exchange_rates.items():\n",
      "    print(f'{currency}:')\n",
      "    for rate_type, rate_value in rates.items():\n",
      "        print(f'  {rate_type}: {rate_value:.4f}')\n",
      "    print()\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
